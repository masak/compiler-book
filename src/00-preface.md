# Preface

Compilers are amazing. On paper, a compiler's job is to take the source code
(in your favorite language) and produce target code (probably some machine code
or byte code). This task splits down into two steps, parsing and code
generation:

    [text] ==PARSING==> ==CODEGEN==> [bytes]

These two steps look quite different: parsing involves building the structure
inherent in the source code, and code generation means taking that structure
and producing the final target code. The output to the parser is the input to
the code generator:

    [text] ==PARSING==> [ast] ==CODEGEN==> [bytes]

"AST" stands for "Abstract Syntax Tree", but don't let that put you off. It's
really just the structured data the parser managed to extract from the source
code, and which the code generator can then use to do its job.

If you're a visually inclined person like me, you can think of the parser as
_raising_ the AST from the flat structure of the source code. The AST has more
depth and structure because the parser is able to identify nesting, hierarchies
etc. from the flat text. But then the codegen again needs to _flatten_ all that
structure into a single (textual or binary) thing. (In functional programming,
raising operation is sometimes called an _anamorphism_, and a flattening
operation is called a _catamorphism_. It's somehow comforting to know that the
theoreticians have been here and slapped Greek words on these things.)

Another set of metaphors that shows up is that of _front end_ and _back end_.
I guess the comparison made here is that of a company. The parser, the front
end, is the user-facing part that can still interact with the user and give
feedback. Customer service requires a certain finesse, a sense of tact, dare
I say empathy and humanity. It matters that the parser can convey useful,
informative, and kindly phrased messages when something is amiss or unclear in
the source code. Great care needs to be taken to make this error handling
user-friendly.

The code generator, however &mdash; the back end &mdash; has no such concerns.
There is no error handling in sight; we're already past the stage where we
have to deal with fickle unreliable customers, or possibly ambiguous source
code; since the AST is generated by a component we trust, it can also be free
of errors. There is no longer any required interaction with the humans, and
the need for tact is gone. Therefore the way the back end works can be brutish,
single-minded, and unrefined. It makes sense; it's much easier to tear
something down than to build something up. The code generator is working with
the flow of entropy.

If the story ended here, compilers would rise to the level of _fascinating_,
but hardly _amazing_.

It's easy to mistakenly describe the job of the compiler as producing the
target code that exactly corresponds to the source code. But if someone came to
you and offered you _two_ different target codes:

* The target code that exactly corresponds to the source code, and

* Another version of the target code, which _does exactly the same_, but twice
  as fast, using less memory, and it's also smaller in size.

You'd be silly not to accept the second one.

In fact, we almost always want to introduce a step in-between parsing and code
generation:

    [text] ==PARSING==> [ast] ==ANALYSIS==> [ast] ==CODEGEN==> [bytes]

All in the name of producing a better AST for the code generator to use. The
actual goal of compilation is not to faithfully reproduce what the programmer
_wrote_ in their source code, but what they _meant_. We want to preserve
_semantics_, not _structure_. In other words, all semantics-preserving
transformations by the analysis step is fair game. We want to find
transformations that make the program better in some way.

(This middle step is often referred to as the _middle end_. I just want to
point out how silly that term is. Whoever chose that name either has a quirky
sense of humor, or is unfamiliar with what an "end" is.)

Maybe the thing that attracts me about the analysis step is that its
temperament is somehow a hybrid between the suave user-friendliness of the
front end/parser, and the ruthless efficiency and mindless goal-orientedness of
the back end/code generator.

I guess what I'm trying to say is that the analysis step is a psycopath.

Let me explain. Let's say you wrote some code and gave it to the compiler. The
analysis step wants to optimize your code. In doing so, it has to _deliberately
misunderstand your exact words_ and substitute what it thinks was your _intent_
in order to produce the most efficient code it can along some dimension. The
analysis step is a duplicitous, self-serving _jerk_ completely without concern
for your original plans and structures.

Before we discard that analogy, let me just say that a well-written analysis
step is a _high-functioning_ psychopath; again, the rules of the game is to
preserve the observable semantics of the program while making better. In other
words, to never get caught actually destroying something. In Perl 6 circles,
the motto is "the compiler is allowed to cheat, as long as it never gets
caught".

High-level languages have gradually provided us with better and better
_abstractions_ to help us build, understand, and change code: functions,
classes, modules, structured control flow, exceptions, macros... The list goes
on. To a surprising extent, it is the job of the analysis step to break down
those carefully built abstractions in the name of speed, code size, and memory
usage. The abstractions are indispensable tools for humans to program with, but
they are mostly removable obstacles when it comes to producing efficient target
code.

Figuring out how to best optimize code is a never-ending proposition: you can
always do better. A theoretical branch of computer science studies
_supercompilation_, the task of finding not just _a_ target code, but _the
most optimal_ target code (in terms of clock cycles) on a given hardware
platform. It does this by an exhaustive search of the entire space of all
possible programs of a given length. Supercompilation is not practical for
normal-sized programs, only for tiny programs that generate a handful of
instructions. Still, two benefits fall out of supercompilation: (a) it hints
that sufficiently advanced optimization of some code is indistinguishable from
search, and (b) sometimes we can learn cool low-level tricks from the output of
a supercompiler, which we can then use in our regular compilers.

XXX Grace Hopper and that text about the idea behind compilation

XXX JIT compilation
